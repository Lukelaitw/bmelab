{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Brain-Computer Interface MLP Classifier\n",
    "For EEG signal relaxation/focus state classification\n",
    "USE RAW DATA AS INPUT\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34603636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Settings\n",
    "class Config:\n",
    "    # Dataset path settings\n",
    "    DATASET_PATH = \"bci_dataset_113-2\"\n",
    "    \n",
    "    # MLP model parameters\n",
    "    HIDDEN_LAYERS = (128, 64, 32)\n",
    "    MAX_ITER = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    ALPHA = 0.001\n",
    "    ACTIVATION = 'relu'\n",
    "    SOLVER = 'adam'\n",
    "    BATCH_SIZE = 64\n",
    "    EARLY_STOPPING = True\n",
    "    VALIDATION_FRACTION = 0.1\n",
    "    N_ITER_NO_CHANGE = 10\n",
    "    \n",
    "    # Signal processing parameters\n",
    "    SAMPLING_RATE = 500\n",
    "    SEGMENT_LENGTH = 5\n",
    "    OVERLAP_RATIO = 0.8 #0.0 ~ 0.8\n",
    "    \n",
    "    # Feature selection parameters\n",
    "    FEATURE_SELECTION = True\n",
    "    # NOTE: When using raw data, N_FEATURES_SELECT will select the top N time points from the segment.\n",
    "    # The number of features is now the number of samples in a segment (SEGMENT_LENGTH * SAMPLING_RATE).\n",
    "    # You may want to adjust this value or disable feature selection.\n",
    "    N_FEATURES_SELECT = 30\n",
    "    \n",
    "    # Other settings\n",
    "    RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b652a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Loading and Processing\n",
    "def load_eeg_data(subject_path):\n",
    "    \"\"\"Load EEG data for a single subject\"\"\"\n",
    "    relax_file = os.path.join(subject_path, \"1.txt\")\n",
    "    focus_file = os.path.join(subject_path, \"2.txt\")\n",
    "    \n",
    "    try:\n",
    "        relax_data = np.loadtxt(relax_file)\n",
    "        focus_data = np.loadtxt(focus_file)\n",
    "        return relax_data, focus_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {subject_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def create_segments(data, segment_length_samples, overlap_samples):\n",
    "    \"\"\"Split continuous EEG signal into multiple segments\"\"\"\n",
    "    if len(data) < segment_length_samples:\n",
    "        return np.array([]) # Return empty array if not enough data for one segment\n",
    "    \n",
    "    segments = []\n",
    "    start = 0\n",
    "    step = segment_length_samples - overlap_samples\n",
    "    \n",
    "    while start + segment_length_samples <= len(data):\n",
    "        segment = data[start:start + segment_length_samples]\n",
    "        segments.append(segment)\n",
    "        start += step\n",
    "    \n",
    "    return np.array(segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Definition and Training\n",
    "class EnhancedBCIClassifier:\n",
    "    \"\"\"Enhanced Brain-Computer Interface Classifier\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = MLPClassifier(\n",
    "            hidden_layer_sizes=Config.HIDDEN_LAYERS,\n",
    "            max_iter=Config.MAX_ITER,\n",
    "            learning_rate_init=Config.LEARNING_RATE,\n",
    "            alpha=Config.ALPHA,\n",
    "            activation=Config.ACTIVATION,\n",
    "            solver=Config.SOLVER,\n",
    "            batch_size=Config.BATCH_SIZE,\n",
    "            early_stopping=Config.EARLY_STOPPING,\n",
    "            validation_fraction=Config.VALIDATION_FRACTION,\n",
    "            n_iter_no_change=Config.N_ITER_NO_CHANGE,\n",
    "            random_state=Config.RANDOM_STATE,\n",
    "            verbose=False\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "        # The number of features k should be less than or equal to the number of samples in a segment\n",
    "        num_features = Config.SAMPLING_RATE * Config.SEGMENT_LENGTH\n",
    "        k_features = min(Config.N_FEATURES_SELECT, num_features)\n",
    "        \n",
    "        self.feature_selector = SelectKBest(f_classif, k=k_features) if Config.FEATURE_SELECTION else None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        # Standardization\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Feature selection\n",
    "        if self.feature_selector is not None:\n",
    "            X_selected = self.feature_selector.fit_transform(X_scaled, y)\n",
    "        else:\n",
    "            X_selected = X_scaled\n",
    "        \n",
    "        # Train the model\n",
    "        self.model.fit(X_selected, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        if self.feature_selector is not None:\n",
    "            X_selected = self.feature_selector.transform(X_scaled)\n",
    "        else:\n",
    "            X_selected = X_scaled\n",
    "            \n",
    "        return self.model.predict(X_selected)\n",
    "    \n",
    "    def get_loss_curve(self):\n",
    "        \"\"\"Get the loss curve\"\"\"\n",
    "        if hasattr(self.model, 'loss_curve_'):\n",
    "            return self.model.loss_curve_\n",
    "        else:\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main Execution Functions\n",
    "def load_all_subjects():\n",
    "    \"\"\"Load data from all subjects\"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_subjects = []\n",
    "    \n",
    "    # Find all subject folders\n",
    "    subject_folders = sorted(glob.glob(os.path.join(Config.DATASET_PATH, \"S*\")))\n",
    "    \n",
    "    if not subject_folders:\n",
    "        print(f\"Error: No subject folders found in {Config.DATASET_PATH}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    for subject_folder in subject_folders:\n",
    "        subject_id = os.path.basename(subject_folder)\n",
    "        \n",
    "        # Load data\n",
    "        relax_data, focus_data = load_eeg_data(subject_folder)\n",
    "        \n",
    "        if relax_data is None or focus_data is None:\n",
    "            continue\n",
    "\n",
    "        # Create signal segments from raw data to be used as input features\n",
    "        segment_length_samples = int(Config.SEGMENT_LENGTH * Config.SAMPLING_RATE)\n",
    "        overlap_samples = int(segment_length_samples * Config.OVERLAP_RATIO)\n",
    "    \n",
    "        relax_segments = create_segments(relax_data, segment_length_samples, overlap_samples)\n",
    "        focus_segments = create_segments(focus_data, segment_length_samples, overlap_samples)\n",
    "        \n",
    "        # Check if segments were created successfully\n",
    "        if relax_segments.size == 0 or focus_segments.size == 0:\n",
    "            print(f\"Warning: Not enough data to create segments for {subject_id}. Skipping this subject.\")\n",
    "            continue\n",
    "            \n",
    "        # The segments are now our \"features\"\n",
    "        relax_features = relax_segments\n",
    "        focus_features = focus_segments\n",
    "        \n",
    "        # Label data\n",
    "        relax_labels = np.zeros(len(relax_features))  # 0 = relax\n",
    "        focus_labels = np.ones(len(focus_features))   # 1 = focus\n",
    "        \n",
    "        # Combine data\n",
    "        subject_features = np.vstack([relax_features, focus_features])\n",
    "        subject_labels = np.hstack([relax_labels, focus_labels])\n",
    "        \n",
    "        # Record subject IDs\n",
    "        subject_ids = [subject_id] * len(subject_labels)\n",
    "        \n",
    "        all_features.append(subject_features)\n",
    "        all_labels.append(subject_labels)\n",
    "        all_subjects.extend(subject_ids)\n",
    "    \n",
    "    if not all_features:\n",
    "        print(\"Error: No valid data found\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Combine all data\n",
    "    X = np.vstack(all_features)\n",
    "    y = np.hstack(all_labels)\n",
    "\n",
    "    return X, y, all_subjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1dc2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def leave_one_subject_out_validation():\n",
    "    \"\"\"Perform Leave-One-Subject-Out cross-validation\"\"\"\n",
    "    print(\"Starting Leave-One-Subject-Out Cross-Validation with RAW DATA...\")\n",
    "    \n",
    "    # Load all data\n",
    "    X, y, subjects = load_all_subjects()\n",
    "    \n",
    "    if X is None:\n",
    "        return None\n",
    "    \n",
    "    # Get unique subject list\n",
    "    unique_subjects = sorted(list(set(subjects)))\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'accuracies': [],\n",
    "        'confusion_matrices': [],\n",
    "        'loss_curves': [],\n",
    "        'subject_names': []\n",
    "    }\n",
    "    \n",
    "    # Test each subject\n",
    "    for test_subject in unique_subjects:\n",
    "        # Split training and test sets\n",
    "        train_mask = [s != test_subject for s in subjects]\n",
    "        test_mask = [s == test_subject for s in subjects]\n",
    "        \n",
    "        X_train, X_test = X[train_mask], X[test_mask]\n",
    "        y_train, y_test = y[train_mask], y[test_mask]\n",
    "        \n",
    "        # Train model\n",
    "        classifier = EnhancedBCIClassifier()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "        \n",
    "        # Store results\n",
    "        results['accuracies'].append(accuracy)\n",
    "        results['confusion_matrices'].append(cm)\n",
    "        results['loss_curves'].append(classifier.get_loss_curve())\n",
    "        results['subject_names'].append(test_subject)\n",
    "        \n",
    "        print(f\"{test_subject}: Accuracy = {accuracy:.3f}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ecb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(results):\n",
    "    \"\"\"Plot result charts\"\"\"\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle('BCI Classifier (Raw Data) - LOSO Cross-Validation Results', fontsize=16)\n",
    "    \n",
    "    # 1. Accuracy distribution\n",
    "    axes[0].bar(range(len(results['accuracies'])), results['accuracies'], \n",
    "                color=['green' if acc >= 0.7 else 'orange' if acc >= 0.6 else 'red' \n",
    "                       for acc in results['accuracies']])\n",
    "    axes[0].set_title('Accuracy by Subject')\n",
    "    axes[0].set_xlabel('Subject Index')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].axhline(y=np.mean(results['accuracies']), color='r', linestyle='--', \n",
    "                    label=f'Mean: {np.mean(results[\"accuracies\"]):.3f}')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    \n",
    "    # 2. Overall confusion matrix\n",
    "    total_cm = np.sum(results['confusion_matrices'], axis=0)\n",
    "    sns.heatmap(total_cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Relax', 'Focus'],\n",
    "                yticklabels=['Relax', 'Focus'],\n",
    "                ax=axes[1])\n",
    "    axes[1].set_title('Overall Confusion Matrix')\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('Actual')\n",
    "    \n",
    "    # 3. Training loss curves\n",
    "    valid_loss_curves = [lc for lc in results['loss_curves'] if len(lc) > 0]\n",
    "    if valid_loss_curves:\n",
    "        for i, loss_curve in enumerate(valid_loss_curves[:5]):  # Show first 5\n",
    "            axes[2].plot(loss_curve, alpha=0.7, label=f'S{i+1}')\n",
    "        axes[2].set_title('Training Loss Curves (First 5 Subjects)')\n",
    "        axes[2].set_xlabel('Iteration')\n",
    "        axes[2].set_ylabel('Loss')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, 'No loss curves available', \n",
    "                     horizontalalignment='center', verticalalignment='center',\n",
    "                     transform=axes[2].transAxes)\n",
    "        axes[2].set_title('Training Loss Curves')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bci_results_raw_data.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd328a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BCI EEG Classification (Raw Data Input) - Relaxation vs Concentration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Perform Leave-One-Subject-Out validation\n",
    "results = leave_one_subject_out_validation()\n",
    "\n",
    "if results is None:\n",
    "    print(\"Validation failed!\")\n",
    "\n",
    "# Display overall results\n",
    "mean_accuracy = np.mean(results['accuracies'])\n",
    "std_accuracy = np.std(results['accuracies'])\n",
    "\n",
    "print(f\"\\nOverall Mean Accuracy: {mean_accuracy:.3f} ± {std_accuracy:.3f}\")\n",
    "\n",
    "# Calculate and display accuracy for each class\n",
    "total_cm = np.sum(results['confusion_matrices'], axis=0)\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    # Calculate accuracy (Recall) for each class\n",
    "    relax_accuracy = total_cm[0, 0] / np.sum(total_cm[0, :]) if np.sum(total_cm[0, :]) > 0 else 0\n",
    "    concentration_accuracy = total_cm[1, 1] / np.sum(total_cm[1, :]) if np.sum(total_cm[1, :]) > 0 else 0\n",
    "    \n",
    "    # Calculate precision for each class\n",
    "    relax_precision = total_cm[0, 0] / np.sum(total_cm[:, 0]) if np.sum(total_cm[:, 0]) > 0 else 0\n",
    "    concentration_precision = total_cm[1, 1] / np.sum(total_cm[:, 1]) if np.sum(total_cm[:, 1]) > 0 else 0\n",
    "\n",
    "print(f\"\\nRelax Class:\")\n",
    "print(f\"  - Accuracy (Recall): {relax_accuracy:.3f} ({total_cm[0, 0]}/{np.sum(total_cm[0, :])})\")\n",
    "print(f\"  - Precision: {relax_precision:.3f} ({total_cm[0, 0]}/{np.sum(total_cm[:, 0])})\")\n",
    "\n",
    "print(f\"\\nConcentration Class:\")\n",
    "print(f\"  - Accuracy (Recall): {concentration_accuracy:.3f} ({total_cm[1, 1]}/{np.sum(total_cm[1, :])})\")\n",
    "print(f\"  - Precision: {concentration_precision:.3f} ({total_cm[1, 1]}/{np.sum(total_cm[:, 1])})\")\n",
    "\n",
    "# Plot results\n",
    "plot_results(results)\n",
    "\n",
    "print(f\"\\nResults saved to 'bci_results_raw_data.png'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
